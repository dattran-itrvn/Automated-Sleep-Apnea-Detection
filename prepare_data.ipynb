{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import mne\n",
    "# import pyedflib\n",
    "import os\n",
    "import glob\n",
    "# import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from lxml import etree\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import multiprocessing as mp\n",
    "from scipy import signal\n",
    "import neurokit2 as nk\n",
    "import pywt\n",
    "import wfdb\n",
    "import wfdb.processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DB_PATH = '/mnt/dat/databases/shhs/polysomnography'\n",
    "TO_PATH = '/mnt/dat/prepped/automated_sleep_apnea'\n",
    "os.makedirs(TO_PATH, exist_ok=True)\n",
    "\n",
    "TARGET_SAMPLING_RATE = 5\n",
    "\n",
    "SEGMENT_LENGTH = 30 # input to model\n",
    "SINGLE_ANNOT = 30 # annotate for every N second\n",
    "SEGMENT_STRIDE = 1 # second\n",
    "\n",
    "HALF_SEGMENTS = (SEGMENT_LENGTH // SINGLE_ANNOT) // 2\n",
    "\n",
    "SAMPLE_RECORD = 'shhs1-200001'\n",
    "\n",
    "SMALL_PORTION = 1 # get small portion of train/validation for debugging or tuning, 1 to ignore\n",
    "\n",
    "MULTIPLE_SPLIT = 5\n",
    "\n",
    "# consider only below channels\n",
    "SELECTED_CHANNELS = ['ABDO RES', 'THOR RES', # resp signals\n",
    "                     'ECG'] # to get EDR (ECG derived resp signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shhs1_edfs = glob.glob(os.path.join(BASE_DB_PATH, 'edfs/shhs1/*.edf'))\n",
    "shhs2_edfs = [] # glob.glob(os.path.join(BASE_DB_PATH, 'edfs/shhs2/*.edf')) # only use shhs1\n",
    "shhs1_edfs = sorted(shhs1_edfs) # only get samples\n",
    "shhs2_edfs = sorted(shhs2_edfs)\n",
    "\n",
    "len(shhs1_edfs), len(shhs2_edfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shhs1_xmls = glob.glob(os.path.join(BASE_DB_PATH, 'annotations-events-nsrr/shhs1/*.xml'))\n",
    "shhs2_xmls = [] # glob.glob(os.path.join(BASE_DB_PATH, 'annotations-events-nsrr/shhs2/*.xml'))\n",
    "shhs1_xmls = sorted(shhs1_xmls)\n",
    "shhs2_xmls = sorted(shhs2_xmls)\n",
    "\n",
    "len(shhs1_xmls), len(shhs2_xmls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine a sample record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edf(path, channels, split_ecg=False, preload=False):\n",
    "    if split_ecg:\n",
    "        resp_edf = mne.io.read_raw_edf(path, include=[c for c in channels if c != \"ECG\"], \n",
    "                                verbose=False, preload=preload)\n",
    "        ecg_edf = mne.io.read_raw_edf(path, include=[\"ECG\"], \n",
    "                                verbose=False, preload=preload)\n",
    "        return resp_edf, ecg_edf\n",
    "\n",
    "    return mne.io.read_raw_edf(path, include=channels, \n",
    "                                verbose=False, preload=preload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_edf, ecg_edf = get_edf([s for s in shhs1_edfs +  shhs2_edfs if SAMPLE_RECORD in s[s.rfind(\"/\")+1:]][0], \n",
    "                            channels=SELECTED_CHANNELS, \n",
    "                            split_ecg=True,\n",
    "                            preload=True)\n",
    "display(resp_edf)\n",
    "display(ecg_edf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_window(array, window_size, axis=-1):\n",
    "    if window_size > array.shape[axis]:\n",
    "        raise ValueError(\"Window size must be smaller than or equal to the size of the specified axis.\")\n",
    "    \n",
    "    start_index = np.random.randint(0, array.shape[axis] - window_size + 1)\n",
    "    \n",
    "    slices = [slice(None)] * array.ndim\n",
    "    slices[axis] = slice(start_index, start_index + window_size)\n",
    "    \n",
    "    # Use slicing to extract the random window\n",
    "    return array[tuple(slices)].copy(), start_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataa1, _ = random_window(resp_edf.get_data(), window_size=int(30 * resp_edf.info['sfreq']))\n",
    "print(dataa1.shape[1])\n",
    "plt.figure(figsize=(12, 4))\n",
    "sns.lineplot(dataa1[0], label=resp_edf.ch_names[0], alpha=0.7)\n",
    "sns.lineplot(dataa1[1], label=resp_edf.ch_names[1], alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataa1, _ = random_window(ecg_edf.get_data(), window_size=int(30 * ecg_edf.info['sfreq']))\n",
    "print(dataa1.shape[1])\n",
    "plt.figure(figsize=(24, 4))\n",
    "sns.lineplot(dataa1[0], label=ecg_edf.ch_names[0], alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def create_annotation(xml_path, start_date, selected_channels=None):\n",
    "    # Use lxml's fast parser\n",
    "    parser = etree.XMLParser(remove_blank_text=True, collect_ids=False)\n",
    "    \n",
    "    # Use xpath for direct access to elements\n",
    "    try:\n",
    "        tree = etree.parse(xml_path, parser)\n",
    "    except (etree.XMLSyntaxError, OSError) as e:\n",
    "        raise ValueError(f\"Error parsing XML file: {e}\")\n",
    "    \n",
    "    onset = []\n",
    "    duration = []\n",
    "    description = []\n",
    "    if selected_channels is not None:\n",
    "        ch_names = []\n",
    "    \n",
    "    for event in tree.xpath(\".//ScoredEvent\"):\n",
    "        event_name = event.find('EventConcept').text\n",
    "        if selected_channels is not None:\n",
    "            ch_name = event.find('SignalLocation')\n",
    "            if ch_name is None:\n",
    "                ch_name = []\n",
    "            elif ch_name.text in selected_channels:\n",
    "                ch_name = [ch_name.text]\n",
    "            else:\n",
    "                ch_name = [] # or ignore by continue\n",
    "\n",
    "        try:\n",
    "            start_time = float(event.findtext('Start', default=\"0.0\"))\n",
    "            sduration = float(event.findtext('Duration', default=\"0.0\"))\n",
    "        except ValueError:\n",
    "            logger.warning(f\"Invalid start time or duration in event: {etree.tostring(event, pretty_print=True).decode()}\")\n",
    "            continue\n",
    "        \n",
    "        onset.append(start_time)\n",
    "        duration.append(sduration)\n",
    "        description.append(event_name)\n",
    "        if selected_channels is not None:\n",
    "            ch_names.append(ch_name)\n",
    "    return mne.Annotations(onset=onset, duration=duration, description=description, \n",
    "                           ch_names=ch_names if selected_channels is not None else None, # ignore ch_names in annot if not specify\n",
    "                           orig_time=start_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anott = create_annotation([s for s in shhs1_xmls + shhs2_xmls if SAMPLE_RECORD in s][0], resp_edf.info['meas_date'])\n",
    "\n",
    "resp_edf = resp_edf.set_annotations(anott)\n",
    "ecg_edf = ecg_edf.set_annotations(anott)\n",
    "\n",
    "anott.to_data_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract EDR from ECG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_respiratory_signal(ecg_signal, wavelet='db4', level=9):\n",
    "    \"\"\"\n",
    "    Extract respiratory signal from ECG using Discrete Wavelet Transform\n",
    "    \n",
    "    Parameters:\n",
    "    - ecg_signal: Input ECG signal\n",
    "    - wavelet: Wavelet type (default: Daubechies 4)\n",
    "    - level: Decomposition level (default: 9)\n",
    "    \n",
    "    Returns:\n",
    "    - respiratory_signal: Reconstructed respiratory signal\n",
    "    - decomposition_coeffs: Full wavelet decomposition coefficients\n",
    "    \"\"\"\n",
    "    # Perform Discrete Wavelet Transform\n",
    "    decomposition_coeffs = pywt.wavedec(ecg_signal, wavelet, level=level)\n",
    "    \n",
    "    # Create a copy of decomposition coefficients for reconstruction\n",
    "    reconstructed_coeffs = [np.zeros_like(coeff) for coeff in decomposition_coeffs]\n",
    "    \n",
    "    # Copy only the detail coefficient at level 9\n",
    "    reconstructed_coeffs[1] = decomposition_coeffs[1]\n",
    "\n",
    "    # Reconstruct the signal using the modified coefficients\n",
    "    respiratory_signal = pywt.waverec(reconstructed_coeffs, wavelet)\n",
    "    \n",
    "    return respiratory_signal\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=1, sharex=True, figsize=(20, 8))\n",
    "\n",
    "ecg_data, _ = random_window(ecg_edf.get_data()[0], window_size=int(30 * ecg_edf.info['sfreq']))\n",
    "sns.lineplot(ecg_data, ax=axes[0], label='ECG')\n",
    "\n",
    "axes[0].set_title(\"ECG\")\n",
    "edr = extract_respiratory_signal(ecg_data, wavelet='db4')\n",
    "sns.lineplot(edr, ax=axes[1], label='Defined method (lv9 decomposition)')\n",
    "\n",
    "axes[1].set_title(\"EDR\")\n",
    "edr1 = nk.ecg_rsp(ecg_data, sampling_rate=ecg_edf.info['sfreq'])\n",
    "sns.lineplot(edr1, ax=axes[1], label='Built-in method (Neurokit2)')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply to whole signal\n",
    "edr_og = extract_respiratory_signal(ecg_edf.get_data()[0])\n",
    "edr_og.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put EDR to Resp signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resample to match with ABDO RES and THOR RES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(12, 5))\n",
    "sns.lineplot(edr_og[:2500], ax=axes[0])\n",
    "\n",
    "edr, _ = wfdb.processing.resample_sig(edr_og, fs=ecg_edf.info['sfreq'], fs_target=resp_edf.info['sfreq'])\n",
    "\n",
    "sns.lineplot(edr[:100], ax=axes[1])\n",
    "\n",
    "plt.show()\n",
    "edr.shape, resp_edf.n_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_channel_to_raw(raw, new_channel_data, channel_name, channel_type):\n",
    "    \"\"\"\n",
    "    Add a new channel to an existing mne.Raw object.\n",
    "\n",
    "    Parameters:\n",
    "    - raw (mne.io.Raw): The original Raw object.\n",
    "    - new_channel_data (numpy.ndarray): The data for the new channel (shape: (n_samples,)).\n",
    "    - channel_name (str): The name of the new channel.\n",
    "    - channel_type (str): The type of the new channel (e.g., 'ecg', 'eog', 'misc').\n",
    "\n",
    "    Returns:\n",
    "    - mne.io.Raw: The Raw object with the new channel added.\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(new_channel_data) != raw.n_times:\n",
    "        raise ValueError(\"new_channel_data must have the same number of samples as the existing Raw data.\")\n",
    "\n",
    "    new_info = mne.create_info([channel_name], sfreq=raw.info['sfreq'], ch_types=[channel_type])\n",
    "\n",
    "    # Reshape the new channel data to (n_channels, n_times)\n",
    "    new_channel_data = new_channel_data[np.newaxis, :]\n",
    "\n",
    "    # Create a RawArray for the new channel\n",
    "    new_raw = mne.io.RawArray(new_channel_data, new_info, verbose=False)\n",
    "\n",
    "    # Add the new channel to the existing Raw object\n",
    "    raw.add_channels([new_raw])\n",
    "\n",
    "    return raw\n",
    "\n",
    "resp_edf = add_channel_to_raw(resp_edf, edr, \"EDR\", resp_edf.info.get_channel_types()[0])\n",
    "resp_edf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def butterworth_lowpass_filter(data, sampling_rate, cutoff_freq=0.7):\n",
    "    \"\"\"\n",
    "    Apply a fourth-order low-pass zero-phase-shift Butterworth filter.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : numpy.ndarray\n",
    "        Input signal to be filtered\n",
    "    cutoff_freq : float\n",
    "        Cut-off frequency in Hz\n",
    "    sampling_rate : float\n",
    "        Sampling rate of the input signal in Hz\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        Filtered signal\n",
    "    \"\"\"\n",
    "    # Nyquist frequency\n",
    "    nyquist_freq = 0.5 * sampling_rate\n",
    "    \n",
    "    # Normalize the cutoff frequency\n",
    "    normal_cutoff = cutoff_freq / nyquist_freq\n",
    "    \n",
    "    # Design the Butterworth filter\n",
    "    b, a = signal.butter(N=4,  # 4th order\n",
    "                         Wn=normal_cutoff,  # normalized cutoff frequency\n",
    "                         btype='low',  # low-pass filter\n",
    "                         analog=False)  # digital filter\n",
    "    \n",
    "    # Apply the filter forward and backward to create zero-phase shift\n",
    "    filtered_signal = signal.filtfilt(b, a, data)\n",
    "    \n",
    "    return filtered_signal\n",
    "\n",
    "\n",
    "# Simulate a respiratory signal\n",
    "sampling_rate = 10  # Hz\n",
    "time = np.linspace(0, 60, 600)  # 60 seconds of data\n",
    "\n",
    "# Create a sample respiratory signal with noise\n",
    "respiratory_signal = (\n",
    "    np.sin(2 * np.pi * 0.25 * time) +  # Respiratory rate ~0.25 Hz\n",
    "    0.5 * np.sin(2 * np.pi * 2 * time) +  # High-frequency noise\n",
    "    np.random.normal(0, 0.1, time.shape)  # Random noise\n",
    ")\n",
    "\n",
    "# Apply the Butterworth filter\n",
    "filtered_signal = butterworth_lowpass_filter(\n",
    "    respiratory_signal, \n",
    "    sampling_rate=sampling_rate,\n",
    "    cutoff_freq=0.7,  # Cut-off at 0.7 Hz \n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(respiratory_signal)\n",
    "plt.plot(filtered_signal)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata = resp_edf.get_data().copy()\n",
    "\n",
    "processed_data = np.apply_along_axis(lambda x: butterworth_lowpass_filter(x, sampling_rate=int(resp_edf.info['sfreq']), cutoff_freq=0.7), \n",
    "                                     axis=1, arr=rawdata)\n",
    "\n",
    "resp_edf._data = processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_length = int(resp_edf.info['sfreq'] * 120)\n",
    "random_segment, index = random_window(rawdata, window_size=sample_length)\n",
    "processed_segment = processed_data[:, index:(index + sample_length)]\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18, 4), sharex=True)\n",
    "\n",
    "axes[0].set_title(\"ABDO RES\")\n",
    "axes[1].set_title(\"THOR RES\")\n",
    "axes[2].set_title(\"EDR\")\n",
    "\n",
    "sns.lineplot(random_segment[0], ax=axes[0], label='OG', alpha=0.8)\n",
    "sns.lineplot(random_segment[1], ax=axes[1], label='OG', alpha=0.8)\n",
    "sns.lineplot(random_segment[2], ax=axes[2], label='OG', alpha=0.8)\n",
    "\n",
    "sns.lineplot(processed_segment[0], ax=axes[0], label='Filterred', alpha=0.6)\n",
    "sns.lineplot(processed_segment[1], ax=axes[1], label='Filterred', alpha=0.6)\n",
    "sns.lineplot(processed_segment[2], ax=axes[2], label='Filterred', alpha=0.6)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_motion_artifacts(signal_data, sampling_rate, window_width=4):\n",
    "    \"\"\"\n",
    "    Remove motion artifacts and baseline wander by subtracting a moving average \n",
    "    filtered signal from the original signal.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    signal_data : numpy.ndarray\n",
    "        Original input signal\n",
    "    sampling_rate : float\n",
    "        Sampling rate of the signal (Hz)\n",
    "    window_width : float, optional\n",
    "        Width of the moving average window in seconds (default is 4 seconds)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        Signal with motion artifacts and baseline wander removed\n",
    "    \"\"\"\n",
    "    # Calculate the window size in samples\n",
    "    window_size = int(window_width * sampling_rate)\n",
    "    \n",
    "    # Ensure window size is odd for symmetric filtering\n",
    "    if window_size % 2 == 0:\n",
    "        window_size += 1\n",
    "    \n",
    "    # Create moving average filter kernel\n",
    "    moving_average_kernel = np.ones(window_size) / window_size\n",
    "    \n",
    "    # Apply moving average filter to the signal\n",
    "    moving_average_signal = np.convolve(\n",
    "        signal_data, \n",
    "        moving_average_kernel, \n",
    "        mode='same'\n",
    "    )\n",
    "    \n",
    "    # Subtract moving average from original signal\n",
    "    artifact_removed_signal = signal_data - moving_average_signal\n",
    "    \n",
    "    return artifact_removed_signal\n",
    "\n",
    "\n",
    "sampling_rate = 10  # Hz\n",
    "time = np.linspace(0, 60, 600)  # 60 seconds \n",
    "original_signal = (\n",
    "    np.sin(2 * np.pi * 0.25 * time) + \n",
    "    0.5 * time +  # Baseline wander (linear drift)\n",
    "    0.3 * np.sin(2 * np.pi * 0.1 * time) +  # Low-frequency motion artifact\n",
    "    np.random.normal(0, 0.1, time.shape)  # Random noise\n",
    ")\n",
    "\n",
    "# Remove motion artifacts\n",
    "cleaned_signal = remove_motion_artifacts(\n",
    "    original_signal, \n",
    "    sampling_rate=sampling_rate,\n",
    "    window_width=4  # 4-second moving average window\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(original_signal)\n",
    "plt.plot(cleaned_signal)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata = resp_edf.get_data().copy()\n",
    "\n",
    "processed_data = np.apply_along_axis(lambda x: remove_motion_artifacts(x, sampling_rate=int(resp_edf.info['sfreq']), window_width=4), \n",
    "                                     axis=1, arr=rawdata)\n",
    "\n",
    "resp_edf._data = processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_length = int(resp_edf.info['sfreq'] * 120)\n",
    "random_segment, index = random_window(rawdata, window_size=sample_length)\n",
    "processed_segment = processed_data[:, index:(index + sample_length)]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18, 4), sharex=True)\n",
    "\n",
    "axes[0].set_title(\"ABDO RES\")\n",
    "axes[1].set_title(\"THOR RES\")\n",
    "axes[2].set_title(\"EDR\")\n",
    "\n",
    "sns.lineplot(random_segment[0], ax=axes[0], label='OG', alpha=0.8)\n",
    "sns.lineplot(random_segment[1], ax=axes[1], label='OG', alpha=0.8)\n",
    "sns.lineplot(random_segment[2], ax=axes[2], label='OG', alpha=0.8)\n",
    "\n",
    "sns.lineplot(processed_segment[0], ax=axes[0], label='Removed noise', alpha=0.6)\n",
    "sns.lineplot(processed_segment[1], ax=axes[1], label='Removed noise', alpha=0.6)\n",
    "sns.lineplot(processed_segment[2], ax=axes[2], label='Removed noise', alpha=0.6)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resample all signal to 5Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_edf.resample(TARGET_SAMPLING_RATE, verbose=False)\n",
    "resp_edf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split to epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The filtered signals are segmented into 30 second epochs with a stride of 1 second between them.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_array_with_indices(arr, axis, \n",
    "                                    single_segment_len=(TARGET_SAMPLING_RATE * SEGMENT_LENGTH), \n",
    "                                    stride=(TARGET_SAMPLING_RATE * SEGMENT_STRIDE)):\n",
    "    \"\"\"\n",
    "    Calculates the indices for overlapping or non-overlapping segments with a specified stride.\n",
    "\n",
    "    Parameters:\n",
    "        arr (np.ndarray): Input array.\n",
    "        axis (int): The axis along which to split.\n",
    "        single_segment_len (int): Length of each segment.\n",
    "        stride (int): Step size between segments.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples containing the start and end indices of each segment.\n",
    "    \"\"\"\n",
    "    axis_len = arr.shape[axis]\n",
    "    num_segments = (axis_len - single_segment_len) // stride + 1\n",
    "\n",
    "    # Generate start and end indices for each segment\n",
    "    segment_indices = []\n",
    "\n",
    "    for i in range(num_segments):\n",
    "        start = i * stride\n",
    "        end = start + single_segment_len\n",
    "        segment_indices.append((start, end))\n",
    "\n",
    "    return np.array(segment_indices)\n",
    "\n",
    "split_index = split_array_with_indices(resp_edf.get_data(), axis=1)\n",
    "split_index.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assign label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"If at the end of an epoch, the sleep technician indicated an apneaic episode, the entire epoch was flagged as a positive apnea episode.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_onset_offset(annot: mne.Annotations, label: str):\n",
    "    adf = annot.to_data_frame()\n",
    "    adf['lower_desc'] = adf['description'].str.lower()\n",
    "    selected_df = adf[adf['lower_desc'].str.contains(label)].copy()\n",
    "    start_time = adf[adf['description'].str.contains('Recording Start Time')]['onset'].iloc[0]\n",
    "    \n",
    "    startt = (selected_df[\"onset\"] - start_time).dt.total_seconds().astype(int)\n",
    "    off = (startt + selected_df[\"duration\"]).astype(int)\n",
    "    return startt, off\n",
    "\n",
    "onsets, offsets = get_onset_offset(resp_edf.annotations, 'pnea') # 'apnea' to include OSA and CSA only, 'pnea' to add hypopnea\n",
    "len(onsets), len(offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_apnea(onset, offset, \n",
    "                 n_times, \n",
    "                 sampling_rate=TARGET_SAMPLING_RATE,\n",
    "                 epoch_len=(TARGET_SAMPLING_RATE * SEGMENT_LENGTH), \n",
    "                 stride=(TARGET_SAMPLING_RATE * SEGMENT_STRIDE)):\n",
    "    \n",
    "    annot = np.zeros(n_times, dtype=bool) # annotation by each sample\n",
    "    \n",
    "    for on, off in zip(onset, offset):\n",
    "        annot[(on * sampling_rate):(off * sampling_rate)] = 1\n",
    "    \n",
    "    split_indices = split_array_with_indices(annot[np.newaxis, :], axis=-1,\n",
    "                                    single_segment_len=epoch_len,\n",
    "                                    stride=stride)\n",
    "    \n",
    "    label = [] # label by each segment\n",
    "    \n",
    "    for start, stop in split_indices:\n",
    "        last_sec = annot[start:stop][-(1 * TARGET_SAMPLING_RATE):]\n",
    "        \n",
    "        if np.all(last_sec == 1): # if entire last 1sec have apnea\n",
    "            label.append(1)\n",
    "        else:\n",
    "            label.append(0)\n",
    "            \n",
    "    return np.array(label)\n",
    "\n",
    "label = assign_apnea(onsets, offsets, resp_edf.n_times)\n",
    "label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(label, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_nonapnea = np.random.choice(np.where(label == 0)[0], size=1)[0]\n",
    "first_apnea = np.random.choice(np.where(label == 1)[0], size=1)[0]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(18, 4), sharex=True)\n",
    "\n",
    "axes[0, 0].set_title(\"ABDO RES\")\n",
    "axes[0, 1].set_title(\"THOR RES\")\n",
    "axes[0, 2].set_title(\"EDR\")\n",
    "\n",
    "x_data = resp_edf.get_data()\n",
    "\n",
    "segment = split_index[first_nonapnea]\n",
    "segment = x_data[:, segment[0]:segment[1]]\n",
    "\n",
    "sns.lineplot(segment[0], ax=axes[0, 0], label='No apnea')\n",
    "sns.lineplot(segment[1], ax=axes[0, 1], label='No apnea')\n",
    "sns.lineplot(segment[2], ax=axes[0, 2], label='No apnea')\n",
    "\n",
    "segment = split_index[first_apnea]\n",
    "segment = x_data[:, segment[0]:segment[1]]\n",
    "\n",
    "sns.lineplot(segment[0], ax=axes[1, 0], color='tab:orange', label='Apnea')\n",
    "sns.lineplot(segment[1], ax=axes[1, 1], color='tab:orange', label='Apnea')\n",
    "sns.lineplot(segment[2], ax=axes[1, 2], color='tab:orange', label='Apnea')\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process on multiple patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shhs1_df = pd.read_csv(os.path.join(BASE_DB_PATH, '../datasets/shhs1-dataset-0.21.0.csv'), \n",
    "                        usecols=['nsrrid', 'weight', \n",
    "                                 'hrdur', 'airdur', 'abdodur', 'chestdur',\n",
    "                                 ],\n",
    "                        index_col='nsrrid')\n",
    "\n",
    "shhs_harmonized = pd.read_csv(os.path.join(BASE_DB_PATH, '../datasets/shhs-harmonized-dataset-0.21.0.csv'), \n",
    "                        usecols=['nsrrid', 'visitnumber', 'nsrr_age', 'nsrr_sex', 'nsrr_bmi', 'nsrr_ahi_hp3r_aasm15', 'nsrr_ttldursp_f1'])\n",
    "\n",
    "shhs1_harmonized = shhs_harmonized[shhs_harmonized['visitnumber'] == 1].copy() # select shhs1 only\n",
    "shhs1_harmonized.set_index('nsrrid', drop=True, inplace=True)\n",
    "shhs1_harmonized.drop(columns=['visitnumber'], inplace=True)\n",
    "\n",
    "shhs1_harmonized['nsrr_ttldursp_f1'] = shhs1_harmonized['nsrr_ttldursp_f1'] / 60 # convert sleep minute to hour\n",
    "\n",
    "\n",
    "shhs1_df = pd.concat([shhs1_df, shhs1_harmonized], axis=1, join='inner')\n",
    "# rename column\n",
    "shhs1_df.rename(columns={'nsrr_ahi_hp3r_aasm15': 'ahi',\n",
    "                         'nsrr_ttldursp_f1': 'sleep_duration'}, inplace=True)\n",
    "\n",
    "shhs1_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select patients with at least 6 hour of useful data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shhs1_df.columns[shhs1_df.columns.str.endswith('dur')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dur_cols = shhs1_df.columns[shhs1_df.columns.str.endswith('dur')]\n",
    "cond = (shhs1_df.loc[:, dur_cols] >= 6).all(axis=1) # at least 6 hour of useful data\n",
    "shhs1_df = shhs1_df[cond].copy()\n",
    "shhs1_df.drop(columns=dur_cols, inplace=True)\n",
    "len(shhs1_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign ahi label\n",
    "bins = [-float('inf'), 5, 15, 30, float('inf')]  # Define bins for ranges\n",
    "labels = ['none', 'mild', 'moderate', 'severe']  # Corresponding labels\n",
    "\n",
    "shhs1_df['ahi_label'] = pd.cut(shhs1_df['ahi'], bins=bins, labels=labels, right=False)\n",
    "shhs1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_record = {}\n",
    "\n",
    "for path in shhs1_xmls + shhs2_xmls:\n",
    "    record = path[path.rfind(\"/\") + 1:path.rfind(\"-nsrr.xml\")]\n",
    "    all_record[record] = [None, path]\n",
    "\n",
    "for path in shhs1_edfs + shhs2_edfs:\n",
    "    record = path[path.rfind(\"/\") + 1:path.rfind(\".edf\")]\n",
    "    all_record[record][0] = path\n",
    "\n",
    "\n",
    "all_record = pd.DataFrame(all_record).T\n",
    "all_record.index.name = 'nsrrid'\n",
    "all_record = all_record.rename(columns={0: 'EDF_path', 1: 'XML_path'})\n",
    "# all_record.reset_index(inplace=True)\n",
    "all_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join path\n",
    "shhs1_df.index = 'shhs1-' + shhs1_df.index.astype(str)\n",
    "\n",
    "shhs1_df = pd.concat([shhs1_df, all_record], axis=1, join='inner')\n",
    "shhs1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare below result with paper (A. Dataset section)\n",
    "\n",
    "print(shhs1_df['nsrr_sex'].value_counts())\n",
    "shhs1_df.describe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shhs1_df.groupby('ahi_label', observed=True)['ahi'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shhs1_df = shhs1_df.loc[:, ['ahi', 'ahi_label', 'sleep_duration', 'EDF_path', 'XML_path']]\n",
    "shhs1_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preload for later process\n",
    "\n",
    "Remove missing records with missing channel (must contain ABDO RES, THOR RES, ECG) and preload label for bootstrapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preload_row(args):\n",
    "    nsrrid, row = args\n",
    "    if row['EDF_path'] is None or row['XML_path'] is None:\n",
    "        return nsrrid, None\n",
    "    try:\n",
    "        resp_edf, ecg_edf = get_edf(row['EDF_path'], SELECTED_CHANNELS, split_ecg=True, preload=False)\n",
    "        if len(resp_edf.ch_names) + len(ecg_edf.ch_names) < 3: # missing channels\n",
    "            return nsrrid, None\n",
    "\n",
    "        anno = create_annotation(row['XML_path'], resp_edf.info['meas_date'], None)\n",
    "        onsets, offsets = get_onset_offset(anno, 'pnea')\n",
    "        target_ntimes = int(resp_edf.n_times * TARGET_SAMPLING_RATE / resp_edf.info['sfreq'])\n",
    "        label = assign_apnea(onsets, offsets, target_ntimes)\n",
    "        \n",
    "        processed_path = os.path.join(TO_PATH, f\"preprocessed/{nsrrid}.npz\")\n",
    "        \n",
    "        if not os.path.exists(processed_path):\n",
    "            resp_edf.load_data(verbose=False)\n",
    "            ecg_edf.load_data(verbose=False)\n",
    "            \n",
    "            edr_og = extract_respiratory_signal(ecg_edf.get_data()[0])\n",
    "            edr, _ = wfdb.processing.resample_sig(edr_og, fs=ecg_edf.info['sfreq'], fs_target=resp_edf.info['sfreq'])\n",
    "            resp_edf = add_channel_to_raw(resp_edf, edr, \"EDR\", resp_edf.info.get_channel_types()[0])\n",
    "            \n",
    "            rawdata = resp_edf.get_data().copy()\n",
    "            processed_data = np.apply_along_axis(lambda x: butterworth_lowpass_filter(x, sampling_rate=int(resp_edf.info['sfreq']), cutoff_freq=0.7), \n",
    "                                                axis=1, arr=rawdata)\n",
    "            processed_data = np.apply_along_axis(lambda x: remove_motion_artifacts(x, sampling_rate=int(resp_edf.info['sfreq']), window_width=4), \n",
    "                                                axis=1, arr=processed_data)\n",
    "            resp_edf._data = processed_data\n",
    "            \n",
    "            resp_edf.resample(TARGET_SAMPLING_RATE, verbose=False)\n",
    "            \n",
    "            x_data = resp_edf.get_data()\n",
    "            \n",
    "            np.savez_compressed(processed_path, x_data)\n",
    "        else:\n",
    "            x_data = np.load(os.path.join(TO_PATH, f\"preprocessed/{nsrrid}.npz\"))['arr_0']\n",
    "        \n",
    "        split_indices = split_array_with_indices(x_data, axis=-1)\n",
    "        \n",
    "        return nsrrid, (x_data, split_indices, label)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing nsrrid {nsrrid}: {e}\")\n",
    "        return nsrrid, None\n",
    "\n",
    "def process_dataframe_in_parallel(df, num_workers=None):\n",
    "    os.makedirs(os.path.join(TO_PATH, \"preprocessed\"), exist_ok=True)\n",
    "    \n",
    "    if num_workers is None:\n",
    "        num_workers = mp.cpu_count() - 4\n",
    "\n",
    "    with mp.Pool(num_workers) as pool:\n",
    "        results = list(tqdm(pool.imap(preload_row, df.iterrows()), total=len(df)))\n",
    "\n",
    "    remove_rows = []\n",
    "    for nsrrid, res in results:\n",
    "        if res is None:\n",
    "            remove_rows.append(nsrrid)\n",
    "        else:\n",
    "            shhs1_df.at[nsrrid, 'X_data'] = res[0]\n",
    "            shhs1_df.at[nsrrid, 'Split'] = res[1]\n",
    "            shhs1_df.at[nsrrid, 'Label'] = res[2]\n",
    "    return remove_rows\n",
    "\n",
    "shhs1_df['X_data'] = None\n",
    "shhs1_df['Split'] = None\n",
    "shhs1_df['Label'] = None\n",
    "\n",
    "remove_rows = process_dataframe_in_parallel(shhs1_df)\n",
    "shhs1_df.drop(index=remove_rows, inplace=True)\n",
    "shhs1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shhs1_df.reset_index(inplace=True)\n",
    "shhs1_df['id'] = shhs1_df['nsrrid'].str.removeprefix(\"shhs1-\").astype(\"int\")\n",
    "shhs1_df.set_index('id', drop=True, inplace=True)\n",
    "shhs1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_df = {'Record_id': [], 'Split': [], 'Label': []}\n",
    "for i, row in shhs1_df.iterrows():\n",
    "    window_df['Split'].extend(row['Split'])\n",
    "    window_df['Label'].extend(row['Label'])\n",
    "    window_df['Record_id'].extend([row.name] * len(row['Label']))\n",
    "    \n",
    "window_df = pd.DataFrame(window_df)\n",
    "window_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_df['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value.encode('utf-8')]))\n",
    "\n",
    "def _float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from an integer or list of integers.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "\n",
    "def serialize_example(ext, label, name=None):\n",
    "    \"\"\"Serialize the X and y data into a TFRecord example.\"\"\"\n",
    "    feature = {'ext': _float_feature(ext.flatten())}\n",
    "    \n",
    "    if label is not None:\n",
    "        feature['label'] = _int64_feature(label.flatten())\n",
    "    if name is not None:\n",
    "        feature['name'] = _bytes_feature(name)\n",
    "        \n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature)).SerializeToString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIONS = tf.io.TFRecordOptions(compression_type=\"GZIP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row(row):\n",
    "    record_id = row['Record_id']\n",
    "    split = row['Split']\n",
    "    y = row['Label']\n",
    "    \n",
    "    x = shhs1_df.loc[record_id]['X_data']\n",
    "    \n",
    "    x = x[:, split[0]:split[1]]\n",
    "    \n",
    "    example = serialize_example(x, np.array([y]))\n",
    "    return example\n",
    "\n",
    "def write_to_tfrecord(queue, output_file):\n",
    "    with tf.io.TFRecordWriter(output_file, options=OPTIONS) as writer:\n",
    "        while True:\n",
    "            item = queue.get()\n",
    "            if item is None:  # Sentinel value to indicate end of data\n",
    "                break\n",
    "            writer.write(item)\n",
    "\n",
    "def main(df: pd.DataFrame, save_name, test=False, num_processes=None, verbose=True):\n",
    "    num_processes = mp.cpu_count() - 4 if num_processes is None else num_processes\n",
    "    output_file = os.path.join(save_name)\n",
    "    \n",
    "    if num_processes > 1:\n",
    "        # Multiprocessing setup\n",
    "        pool = mp.Pool(processes=num_processes)\n",
    "        manager = mp.Manager()\n",
    "        queue = manager.Queue(maxsize=1000)  # Limit queue size to avoid OOM\n",
    "        writer_process = mp.Process(target=write_to_tfrecord, args=(queue, output_file))\n",
    "        writer_process.start()\n",
    "\n",
    "        iterator = pool.imap(process_row, (row for _, row in df.iterrows()))\n",
    "        if verbose:\n",
    "            iterator = tqdm(iterator, desc=f\"Preparing {os.path.basename(save_name)}\", total=len(df))\n",
    "        \n",
    "        for result in iterator:\n",
    "            queue.put(result)\n",
    "\n",
    "        # Signal the writer process to finish\n",
    "        queue.put(None)\n",
    "        writer_process.join()\n",
    "\n",
    "        # Close and join the pool\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "    \n",
    "    else:\n",
    "        # Single-process setup\n",
    "        with tf.io.TFRecordWriter(output_file, options=OPTIONS) as writer:\n",
    "            iterator = (process_row(row) for _, row in df.iterrows())\n",
    "            if verbose:\n",
    "                iterator = tqdm(iterator, total=len(df))\n",
    "            \n",
    "            for example in iterator:\n",
    "                writer.write(example)\n",
    "    \n",
    "    return f\"File {save_name} completed.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_states = random.sample(range(1, 100 + 1), MULTIPLE_SPLIT)\n",
    "random_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shhs1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for state in random_states:\n",
    "    # shhs1_df = shhs1_df.sample(frac=SMALL_PORTION, random_state=state)\n",
    "    window_df = window_df[window_df['Record_id'].isin(shhs1_df.index)]\n",
    "    window_df = window_df.sample(frac=1, random_state=state) # shuffle\n",
    "    \n",
    "    train_df, test_df = train_test_split(shhs1_df, train_size=100, \n",
    "                                                random_state=state, \n",
    "                                                stratify=shhs1_df['ahi_label']) # should use AHI\n",
    "\n",
    "    test_window_df = window_df[window_df['Record_id'].isin(test_df.index)]\n",
    "    train_window_df = window_df[window_df['Record_id'].isin(train_df.index)]\n",
    "\n",
    "    zero_df = train_window_df[train_window_df['Label'] == 0]\n",
    "    one_df = train_window_df[train_window_df['Label'] == 1]\n",
    "    \n",
    "    n_datasets = len(zero_df) // len(one_df)\n",
    "    sub_datasets = []\n",
    "    for i in range(n_datasets):\n",
    "        start = i * len(one_df)\n",
    "        stop = min((i + 1) * len(one_df), len(zero_df))\n",
    "        \n",
    "        subset = pd.concat([zero_df.iloc[start:stop], one_df], axis=0)\n",
    "        sub_datasets.append(subset)\n",
    "    \n",
    "    for i, subset in enumerate(sub_datasets):\n",
    "        train_set, validation_set = train_test_split(subset, test_size=0.2, \n",
    "                                                        random_state=state)\n",
    "        \n",
    "        names = [\"train\", \"val\"]\n",
    "        if SMALL_PORTION != 1:\n",
    "            names = [n + \"_sub\" for n in names]\n",
    "        names = [f\"{n}_state_{state}_part_{i}.tfrecord\" for n in names]\n",
    "        \n",
    "        if len(train_set):\n",
    "            result = main(train_set, os.path.join(TO_PATH, names[0]), num_processes=mp.cpu_count() - 4)\n",
    "            # print(result)\n",
    "        \n",
    "        if len(validation_set):\n",
    "            result = main(validation_set, os.path.join(TO_PATH, names[1]), num_processes=mp.cpu_count() - 4)\n",
    "            # print(result)\n",
    "    \n",
    "    # if len(test_window_df):\n",
    "    #     result = main(test_window_df, os.path.join(TO_PATH, f\"test_state_{state}.tfrecord\"), num_processes=mp.cpu_count() - 4)\n",
    "        # print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
